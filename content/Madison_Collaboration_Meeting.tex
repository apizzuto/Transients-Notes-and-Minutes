\chapter{Madison Collaboration Meeting, Neutrino Sources}

\section{Parallel Session Number One}
\begin{itemize}
    \item \textbf{Overview on Tools and Analyses (Josh)}: Gave an overview of all of the tools we can use to do point source analyses. Began a discussion on trials correction for all of the different analyses
    \item \textbf{Path towards new PS Likelihood (Theo, Hans, and Martin)}: Starting from first principles, rederives PS likelihood. They extract PDFs from Monte Carlo using KDE techniques, trying to be smart about the high dimensionality. They argue that per event their rigorous treatment reduces to only two spline evaluations. Nice feature is that the KDE seems able to localize sources. 
    \item \textbf{Determining Time Dependence (Ignacio, on behalf of Pranav)}: Taking an already found signal in time-integrated data and determining if there is underlying temporal behavior. The TS is essentially the magnitude of the difference between the CDF of the coincident events as a function of time vs a constant rise hypothesis (steady-source hypothesis). Shows comparison to the ideal cases (injected at a delta function at the very beginning or very end of the livetime). \emph{Question}: there are some background events that contribute, and their behavior isn't truly time independent. Would this be possible to include? Ignacio says that this wouldn't be hard to implement.
    \item \textbf{Periodic Neutrino Sources (Chun Fai Tung)}: X-Ray and Gamma-ray binaries are expected to be periodic sources. 4 proposed searches: triggered with fixed period, catalog with unknown period, time-integrated catalog search, and a population analysis. He shows how well his method can recover flares with different underlying periodicities, but is worried about computational costs.
    \item \textbf{Search for Binaries (Qinrui)}: Investigating microquasars (binaries with jets) and some other binary systems. Difference between this and Chun Fai's analysis is that this analyses uses lightcurves from Swift-BAT (and then apply Bayesian Blocks to identify flares) to build the time PDFs. Sourcelist from WATCHDOG (BH X-ray binary list from the last 20 years). Qinrui shows some sensitivities and discovery potentials for a few examples of light curves. Qinrui is currently investigating the best way to include sources of interest that are not included in WATCHDOG.
\end{itemize}

\section{Parallel Session Number Two}
\begin{itemize}
    \item \textbf{Tidal Disruption Events (Robert)}: Robert showed an overview of his TDE analysis as he is working towards a paper outline. Analysis method fits source weights individually, in case TDEs are not standard candles. Unblinded p-values are consistent with background. Some TDEs were analyzed individually, again, consistent with background. Limit calculation assumes a weighting scheme. Robert discusses prospects for sensitivities and rate estimation with new detections of TDEs, especially from ZTF, and so he hopes to update the analysis in the future. \emph{Questions}: Markus asked how you can set such constraining limits (below the diffuse flux) with only a few TDEs. I asked about how much the assumption of stadard candles changes the limits.
    \item \textbf{Extended Source Analysis Update (Devyn)}: Many extended sources with few-degree extensions detected by HESS and HAWC in high energies. For source extensions as small as 2 degrees, sensitivity is order of magnitude better than PS likelihood when extension is taken into account. Devyn shows that she does not reproduce a bug(?) from the historic analysis when she is rerunning it. Devyn is puzzled by the fact that signal subtracted likelihood is pushing the background distribution in the wrong direction. With different background parameterization, the behavior is as expected, but there is some odd behavior now when comparing the KDE and Splined BG TS distributions. Devyn also summarizes Mehr's work in Skylab with Tessa's sample. Some questions were raised about odd structures in zenith PDFs. 
    \item \textbf{Untriggered, time-dependent blazar search (Erin)}: Erin presents an unblinding request for the untriggered flare analysis on the Fermi 3LAC catalog using the diffuse sample. Significance will be calculated using a binomial test of the population (with correction for correlations between blazars). Erin mainly focused the presentation on the decorrelation method that she is suggesting because the rest of the analysis is using standard methods. \emph{Questions}: Greg asked if you need to be so severe as to remove an entire blazar when decorrelating instead of just assigning events to single blazars and not both.
    \item \textbf{GRB Precursor and Afterglows (Kunal)}: Kunal mentions that historic GRB analyses only considered the prompt phase of GRBs, and he is interested in extending to the precursor ($\mathcal{O}$(weeks)) and the afterglow ($\mathcal{O}$(months)). He chooses the data to inform him how long of a time window he can afford to look at for the precursor and afterglow. He shows a discovery potential curve as a function of the time window. I'm concerned that he is extrapolating his TS distribution according to a $\chi ^2$ that doesn't fit well, may just want to use a 3$\sigma$ discovery potential. \emph{Questions} What is the confidence level for the discovery potential? 50 percent. Markus brought up that the two discovery potential curves on slide 7 should be identical if just a simple box is used, but Kunal explains that the time window is fit for, $T_0$ just reflects the injected signal timescale. Ignacio is concerned that the proper likelihood is not being used, and suggests iterating offline. 
    \item \textbf{GRB Precursors (Paul)}: Paul wants to used identified precursors, which makes his analysis separate from Kunal's which fits for emission timescales for all GRBs. Paul wants to use the standard transient likelihood, but just with larger time windows to include the precursor stage, using GFU and a list of GRBs from GRBweb with identified precursors. For GRBweb, all Paul needs to do is move the SQL database from his local machine to the Madison servers. Paul has a way to obtain healpix maps of GRBs detected by Fermi-GBM, and he compares to the mismatch that was obtained when treating the burst uncertainties as 2D-gaussians. Paul then discusses identifying precursors, which he has to do himself. He is using a Bayesian Blocks approach on the public Fermi-GBM data, and is trying to reproduce old Fermi-GBM results. \emph{Questions}: Are you analyzing just the precursor time window or doing the analysis from the beginning of the precursor until the beginning of the prompt phase. This study will be done.
    \item \textbf{GRB Coincidence Analysis (Liz)}: Liz presents an update on a per GRB analysis from 2011-present with the hopes of eventually scaling to a realtime analysis. Liz is suggesting using the GRB $T_{100}$ and then adding extra time to the time window in an expanding time window method. She shows the sensitivity of the analyses for different time windows and different temporal patterns for signal injection for a test case at a declination of $\delta=+11.5^{\circ}$ with a $T_{100}=$24.6 s. Liz calculates the effective trials factor for correlated expanding time windows. She suggests comparing the p-value from the $T_{100}$ window and comparing it to the best of the rest of the expanding time windows.
\end{itemize}

\section{Parallel Session Number Three}
\begin{itemize}
    \item \textbf{ESTES Event Selection (Manuel on behalf of Sarah)}: ``Saruel'' presents a technical overview of the ESTES event selection. ESTES defines an event specific dark region to declare the veto, which increases the effective volume. Veto is determined by starting at the first lit DOM, looking at photon tables, and calculating a probability that a muon could have snuck by to that DOM without making any previous deposits. \emph{Questions}: Ignacio asked if there is an effective minimum number of outer layers that function as a veto. Manuel says no, as few as one is fine.
    \item \textbf{Neutrino Source Cascade Dataset (Steve)}: Steve shows progress he has made on a cascade nu-sources event selection. Improvement in southern sky because cascades do not require a large energy threshold to cut out background. Historic MESE sample relies on hardcuts, goal is to not require hardcuts and veto to improve effective volume, and use the DNN reconstruction for the events. Angular resolution from DNN trained on cscdl3. Method currently performs better than MESC at low energies, worse at higher energies. Improvement in sensitivities for softer spectra, no gains in hard spectra because of this behavior. \emph{Questions}: Do you think the estimated errors are good? Steve has not yet checked Data/MC agreement nor how accurately the Gaussian approximation works.
    \item \textbf{Multiflare Stacking (Will)}: Will began with a refresher of his multiflare analysis. Will is using a binomial approach to stacking flares, which is essentially a multiflare extension of Erin's single-flare binomial stacking. Will has begun working on multiflare skymaps, i.e., not using a source list and just performing his analysis on the entire sky. This is a ``Computational Nightmare,'' so Will is debating moving the analysis to cSky to accomplish this part of the analysis. Will also points out that multiflare stacking is complementary, not meant to replace, single flare analyses.
    \item \textbf{ZTF-IceCube Analyses (Anna on behalf of Ludwig)}: Of the last 7 IceCube alerts, only 1 could be observed (a doublet at the declination $\delta = +13^{\circ}$). Estimates for Gold events coverage is about 10 per year. Analysis considers the whole GFU stream as well as all current transients from ZTF. If deemed interesting, you can trigger spectroscopy follow up in ZTF. Filters transients brighter than 19th mag, complete up to 18.5 mag. Anna describes the logic behind triggering observations.
    \item \textbf{Self-triggered PS analysis (Martina)}: Martina proposes using the TXS analysis on all alert events, i.e. using the HESE tracks and EHE events as a ``source catalog.'' Sky is divided up into Cartesian grid positions with a width of $0.1$ degrees. Martina suggests at each source doing a time-integrated search and a time dependent (single flare) search. She suggests doing both a stacked and max-burst analysis for the continuous emission, and for the flare followup only doing a max-burst search. \emph{Questions:} Questions were brought up about trials factor and how Martina is taking into account the uncertainty information on the alert events. Qinrui asked about whether or not the signalness is taken into account. Justin asked if there is overlap with the fast response analyses. What Martina calls the ``transient'' analysis is the untriggered flare fit. I wonder why she doesn't take the full likelihood information into account, which we have for the events.
    \item \textbf{UHECR Analysis (Lisa)}: Lisa presents an unblinding request for her UHECR analysis. This is mainly an extension to the analysis that was unblinded before Stockholm, namely, with larger datasets. Lisa is also rerunning some signal injection(?) in a slightly different manner, but beyond that, the analysis is very similar and meant as a complement to Anastasia's analysis.
\end{itemize}

\section{Gong Talks}
\begin{itemize}
    \item \textbf{Mixed proton-iron composition in UHECR analysis (Lisa)}: Lisa presented an update on Phillip's work on how a different assumption on the cosmic ray compisition would affect the UHECR-nu correlation analyses (specifically with the size of the uncertainty regions because different elements have different rigidities)
    \item \textbf{Novae with GRECO (me)}: I showed some signal and background estimates for novae assuming negligible gamma-ray absorption
    \item \textbf{Upper limit calculation for GW-nu analyses (Doga)}: Doga discussed how to appropriately set limits in a way that does not take into account irrelevant information
    \item \textbf{Neutrinos from AGN cores (Robert on behalf of Federica)}: Says that accretion disks are prime candidates for neutrino production, assuming neutrino production is proportional to luminosity of AGN cores. There are a lot of sources, Federica is looking into how to deal with this.
    \item \textbf{SkyLLH (Martin)}: SkyLLH sales pitch.
    \item \textbf{Counting experiment sensitivity in different background regimes (Me)}: I talked about a weird dip feature that shows up in low background analyses when integrating over a small but non-negligible amount of background
    \item \textbf{Kinematic angle (Paul)}: Paul actually does the full calculation for the kinematic opening angle for charged current interactions. He especially notes the long tails of these distributions that are often neglected.
    \item \textbf{Datasets update (Liz)}: To unblind, the data curator now needs to approve the dataset. Approved datasets have the advantage of not needing to go through this procedure. Liz suggests buffering unblinding by about a week to account for this process if you are not using an approved dataset
    \item \textbf{Optimizing optical followups (Robert)}: Robert suggests being proactive, ie, defining if you want a pure, large, biased, or nearby sample. To do this, you should simulate the source population before just going after one.
    \item \textbf{Realtime alerts from HESE Cascades (Hugo)}: It can be helpful to include HESE cascades for short timescale transients. Hugo presents some preliminary work towards this.
    \item \textbf{HESE Skymap scans (Robert)}: The HESE team needs sky scans of the HESE events (overall about 30 scans need to be done), and Robert is working on them. These scans are slightly different from the realtime scans, and Robert notes the differences between them.
    \item \textbf{Hydrangea Followup (Sarah)}: Sarah wants to followup Hydrangea using the same methods as the transient search for ANITA. \emph{Questions}: Why no time-integrated search?
    \item \textbf{Exploring counterparts of high energy neutrino events (Lisa on behalf of Pratush)}: Lisa discusses a proposed cross-correlation analyses between the Swift-BAT catalog and IceCube data 
    \item \textbf{X-ray and UV/O search from joint GW-nu coincidences (Azadeh)}: Azadeh discusses Swift Follow-up observations of joint GW-nu coincidences. Azadeh was appointed 4 priority 1 approved triggers, and will trigger on coincidences with FAR of no more than 2 yr$^{-1}$.
    \item \textbf{Spline MPE/Paraboloid (Jimmy)}: Jimmy airs our dirty laundry behind Spline MPE and Paraboloid likelihood spaces. He especially notes that Wilk's breaks down at high energies from an increase in stochastic losses. Minimizer fails to find the global minimum around 10$\%$ of the time
    \item \textbf{Blazar stacking limits (Matthias)}: Matthias unblinded the analysis correlating with the 3FHL catalog. Limits are at the 10-20$\%$ level of the diffuse flux for different assumed spectra. He proposes an update that would use 4FGL and more IceCube data, and additionally fitting for the distribution of blazar spectral indeces.
    \item \textbf{DECam search for optical transients (Rob and Keith)}: Rob discusses his analysis searching for explosive optical transients in coincidence with TeV-PeV neutrinos. DECam gets down to about 23-24 mag, so they can probe really deep sources, specifically looking for core-collapse supernovae. Rob shows that signal and background is separable on a statistical basis
    \item \textbf{Astrophysical neutrino catalog (Ignacio on behalf of Charles)}: The idea is to have a website with all of the information about published, likely astrophysical, neutrinos.
    \item{ \textbf{Next Steps in FRB Analyses (Justin)}}: Justin talked about FRB detection rates as well as prospects for future sensitivities with a larger source class.
\end{itemize}